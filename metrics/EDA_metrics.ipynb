{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLLB Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nllb_df = pd.read_csv('nllb_translation_metrics.csv')\n",
    "nllb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column that has the number of words for en, es, predicted_es, and predicted_en\n",
    "nllb_df['en_word_count'] = nllb_df['en'].str.split().str.len()\n",
    "nllb_df['es_word_count'] = nllb_df['es'].str.split().str.len()\n",
    "nllb_df['predicted_es_word_count'] = nllb_df['predicted_es'].str.split().str.len()\n",
    "nllb_df['predicted_en_word_count'] = nllb_df['predicted_en'].str.split().str.len()\n",
    "\n",
    "# make a new column that has the number of characters for en, es, predicted_es, and predicted_en\n",
    "nllb_df['en_char_count'] = nllb_df['en'].str.len()\n",
    "nllb_df['es_char_count'] = nllb_df['es'].str.len()\n",
    "nllb_df['predicted_es_char_count'] = nllb_df['predicted_es'].str.len()\n",
    "nllb_df['predicted_en_char_count'] = nllb_df['predicted_en'].str.len()\n",
    "\n",
    "nllb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average number of words in the en, es, predicted_es, and predicted_en columns\n",
    "avg_word_count = nllb_df[['en_word_count', 'es_word_count', 'predicted_es_word_count', 'predicted_en_word_count']].mean()\n",
    "print(avg_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average percentage of overall predicted words relative to the actual words\n",
    "print(avg_word_count['predicted_es_word_count'] / avg_word_count['es_word_count'])\n",
    "print(avg_word_count['predicted_en_word_count'] / avg_word_count['en_word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Create a smoothing function\n",
    "smoothie = SmoothingFunction(k=1).method7  # You can experiment with different methods (method1 through method7)\n",
    "\n",
    "# Update your calculation to use the smoothing function\n",
    "nllb_df['es_bleu'] = nllb_df.apply(lambda row: sentence_bleu([row['es'].split()], row['predicted_es'].split(), smoothing_function=smoothie), axis=1)\n",
    "nllb_df['en_bleu'] = nllb_df.apply(lambda row: sentence_bleu([row['en'].split()], row['predicted_en'].split(), smoothing_function=smoothie), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average BLEU score for the predicted_es and predicted_en columns\n",
    "avg_bleu = nllb_df[['es_bleu', 'en_bleu']].mean()\n",
    "print(f'Average BLEU score for predicted_es: {avg_bleu[\"es_bleu\"]}')\n",
    "print(f'Average BLEU score for predicted_en: {avg_bleu[\"en_bleu\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average time it takes to translate en to es and es to en\n",
    "avg_es_time = nllb_df['prediction_time_es'].mean()\n",
    "avg_en_time = nllb_df['prediction_time_es'].mean()\n",
    "print(f'Average time to translate en to es: {avg_es_time}')\n",
    "print(f'Average time to translate es to en: {avg_en_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the upper bound and the lower of the average time it takes to translate en to es and es to en\n",
    "upper_es_time = nllb_df['prediction_time_es'].max()\n",
    "lower_es_time = nllb_df['prediction_time_es'].min()\n",
    "upper_en_time = nllb_df['prediction_time_en'].max()\n",
    "lower_en_time = nllb_df['prediction_time_en'].min()\n",
    "print(f'Upper bound time to translate en to es: {upper_es_time}')\n",
    "print(f'Lower bound time to translate en to es: {lower_es_time}')\n",
    "print(f'Upper bound time to translate es to en: {upper_en_time}')\n",
    "print(f'Lower bound time to translate es to en: {lower_en_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average time it takes to translate en to es and es to en relative to the number of words\n",
    "nllb_df['prediction_time_es_per_word'] = nllb_df['prediction_time_es'] / nllb_df['en_word_count']\n",
    "nllb_df['prediction_time_en_per_word'] = nllb_df['prediction_time_en'] / nllb_df['es_word_count']\n",
    "avg_es_time_per_word = nllb_df['prediction_time_es_per_word'].mean()\n",
    "avg_en_time_per_word = nllb_df['prediction_time_en_per_word'].mean()\n",
    "print(f'Average time to translate en to es per word: {avg_es_time_per_word}')\n",
    "print(f'Average time to translate es to en per word: {avg_en_time_per_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average time it takes to translate en to es and es to en relative to the number of characters\n",
    "nllb_df['prediction_time_es_per_char'] = nllb_df['prediction_time_es'] / nllb_df['en_char_count']\n",
    "nllb_df['prediction_time_en_per_char'] = nllb_df['prediction_time_en'] / nllb_df['es_char_count']\n",
    "avg_es_time_per_char = nllb_df['prediction_time_es_per_char'].mean()\n",
    "avg_en_time_per_char = nllb_df['prediction_time_en_per_char'].mean()\n",
    "print(f'Average time to translate en to es per char: {avg_es_time_per_char}')\n",
    "print(f'Average time to translate es to en per char: {avg_en_time_per_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average wer for the predicted_es and predicted_en columns\n",
    "avg_wer_en = nllb_df['WER_en'].mean()\n",
    "avg_wer_es = nllb_df['WER_es'].mean()\n",
    "print(f'Average WER for predicted_en: {avg_wer_en}')\n",
    "print(f'Average WER for predicted_es: {avg_wer_es}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average wer for the predicted_es and predicted_en columns relative to the number of words\n",
    "nllb_df['WER_en_per_word'] = nllb_df['WER_en'] / nllb_df['en_word_count']\n",
    "nllb_df['WER_es_per_word'] = nllb_df['WER_es'] / nllb_df['es_word_count']\n",
    "avg_wer_en_per_word = nllb_df['WER_en_per_word'].mean()\n",
    "avg_wer_es_per_word = nllb_df['WER_es_per_word'].mean()\n",
    "print(f'Average WER for predicted_en per word: {avg_wer_en_per_word}')\n",
    "print(f'Average WER for predicted_es per word: {avg_wer_es_per_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average cer for the predicted_es and predicted_en columns\n",
    "avg_cer_en = nllb_df['CER_en'].mean()\n",
    "avg_cer_es = nllb_df['CER_es'].mean()\n",
    "print(f'Average CER for predicted_en: {avg_cer_en}')\n",
    "print(f'Average CER for predicted_es: {avg_cer_es}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average mer for the predicted_es and predicted_en columns\n",
    "avg_mer_en = nllb_df['MER_en'].mean()\n",
    "avg_mer_es = nllb_df['MER_es'].mean()\n",
    "print(f'Average MER for predicted_en: {avg_mer_en}')\n",
    "print(f'Average MER for predicted_es: {avg_mer_es}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert from transformers to check the sentiment similarity between the en and predicted_en columns\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "nllb_df['en_sentiment'] = nllb_df['en'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "nllb_df['predicted_en_sentiment'] = nllb_df['predicted_en'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "nllb_df['en_sentiment_score'] = nllb_df['en'].apply(lambda x: sentiment_analysis(x)[0]['score'])\n",
    "nllb_df['predicted_en_sentiment_score'] = nllb_df['predicted_en'].apply(lambda x: sentiment_analysis(x)[0]['score'])\n",
    "nllb_df['sentiment_similarity_en'] = nllb_df.apply(lambda row: row['en_sentiment'] == row['predicted_en_sentiment'], axis=1)\n",
    "sentiment_similarity_en = nllb_df['sentiment_similarity_en'].mean()\n",
    "print(f'Sentiment similarity between en and predicted_en: {sentiment_similarity_en}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert from transformers to check the sentiment similarity between the es and predicted_es columns\n",
    "nllb_df['es_sentiment'] = nllb_df['es'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "nllb_df['predicted_es_sentiment'] = nllb_df['predicted_es'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "nllb_df['es_sentiment_score'] = nllb_df['es'].apply(lambda x: sentiment_analysis(x)[0]['score'])\n",
    "nllb_df['predicted_es_sentiment_score'] = nllb_df['predicted_es'].apply(lambda x: sentiment_analysis(x)[0]['score'])\n",
    "nllb_df['sentiment_similarity_es'] = nllb_df.apply(lambda row: row['es_sentiment'] == row['predicted_es_sentiment'], axis=1)\n",
    "sentiment_similarity_es = nllb_df['sentiment_similarity_es'].mean()\n",
    "print(f'Sentiment similarity between es and predicted_es: {sentiment_similarity_es}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tts_en = pd.read_csv('tts_metrics_en.csv')\n",
    "df_tts_en['tts_time'] = df_tts_en['tts_time'] \n",
    "df_tts_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the audios length of \"path\" and \"audio\" columns\n",
    "import librosa\n",
    "df_tts_en['audio_length'] = df_tts_en['path'].apply(lambda x: librosa.get_duration(filename=x))\n",
    "df_tts_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in the folder \"orignals_en\" and get the length of each audio in order and append them to the df_tts_en dataframe in a new column called \"original_audio_length\" in the same order\n",
    "# their names are result_{i}.wav where i is the index of the row\n",
    "import os\n",
    "import numpy as np\n",
    "audio_lengths = []\n",
    "for i in range(len(df_tts_en)):\n",
    "    audio_path = f'originals_en/result_{i}.wav'\n",
    "    audio_lengths.append(librosa.get_duration(filename=audio_path))\n",
    "df_tts_en['predicted_audio_length'] = audio_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average audio length of the \"audio_length\" and \"predicted_audio_length\" columns\n",
    "avg_audio_length = df_tts_en[['audio_length', 'predicted_audio_length']].mean()\n",
    "avg_audio_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average percentage of the predicted audio length relative to the original audio length\n",
    "print(avg_audio_length['predicted_audio_length'] / avg_audio_length['audio_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average similarity column as it is beteen 0 and 1\n",
    "avg_similarity = df_tts_en['similarity'].mean()\n",
    "print(f'Average similarity: {avg_similarity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new columns for the number of words and characters in the \"transcription\" column\n",
    "df_tts_en['transcription_word_count'] = df_tts_en['transcription'].str.split().str.len()\n",
    "df_tts_en['transcription_char_count'] = df_tts_en['transcription'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average words and characters in the \"transcription\" column\n",
    "avg_transcription_word_count = df_tts_en['transcription_word_count'].mean()\n",
    "avg_transcription_char_count = df_tts_en['transcription_char_count'].mean()\n",
    "print(f'Average transcription word count: {avg_transcription_word_count}')\n",
    "print(f'Average transcription char count: {avg_transcription_char_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average tts_time column\n",
    "avg_tts_time = df_tts_en['tts_time'].mean()\n",
    "print(f'Average tts time: {avg_tts_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average tts_time column relative to the number of words\n",
    "df_tts_en['tts_time_per_word'] = df_tts_en['tts_time'] / df_tts_en['transcription_word_count']\n",
    "avg_tts_time_per_word = df_tts_en['tts_time_per_word'].mean()\n",
    "print(f'Average tts time per word: {avg_tts_time_per_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average tts_time column relative to the number of characters\n",
    "df_tts_en['tts_time_per_char'] = df_tts_en['tts_time'] / df_tts_en['transcription_char_count']\n",
    "avg_tts_time_per_char = df_tts_en['tts_time_per_char'].mean()\n",
    "print(f'Average tts time per char: {avg_tts_time_per_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the upper and lower bounds of the tts_time column\n",
    "upper_tts_time = df_tts_en['tts_time'].max()\n",
    "lower_tts_time = df_tts_en['tts_time'].min()\n",
    "print(f'Upper bound tts time: {upper_tts_time}')\n",
    "print(f'Lower bound tts time: {lower_tts_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tts_es = pd.read_csv('tts_metrics_es.csv')\n",
    "df_tts_es['tts_time'] = df_tts_es['tts_time'] \n",
    "df_tts_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the audios length of \"path\" and \"audio\" columns\n",
    "df_tts_es['audio_length'] = df_tts_es['path'].apply(lambda x: librosa.get_duration(filename=x))\n",
    "df_tts_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in the folder \"orignals_es\" and get the length of each audio in order and append them to the df_tts_es dataframe in a new column called \"original_audio_length\" in the same order\n",
    "# their names are result_{i}.wav where i is the index of the row\n",
    "audio_lengths = []\n",
    "for i in range(len(df_tts_es)):\n",
    "    audio_path = f'originals_es/result_{i}.wav'\n",
    "    audio_lengths.append(librosa.get_duration(filename=audio_path))\n",
    "df_tts_es['predicted_audio_length'] = audio_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average audio length of the \"audio_length\" and \"predicted_audio_length\" columns\n",
    "avg_audio_length = df_tts_es[['audio_length', 'predicted_audio_length']].mean()\n",
    "avg_audio_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average percentage of the predicted audio length relative to the original audio length\n",
    "print(avg_audio_length['predicted_audio_length'] / avg_audio_length['audio_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average similarity column as it is beteen 0 and 1\n",
    "avg_similarity = df_tts_es['similarity'].mean()\n",
    "print(f'Average similarity: {avg_similarity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new columns for the number of words and characters in the \"transcription\" column\n",
    "df_tts_es['transcription_word_count'] = df_tts_es['transcription'].str.split().str.len()\n",
    "df_tts_es['transcription_char_count'] = df_tts_es['transcription'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average words and characters in the \"transcription\" column\n",
    "avg_transcription_word_count = df_tts_es['transcription_word_count'].mean()\n",
    "avg_transcription_char_count = df_tts_es['transcription_char_count'].mean()\n",
    "print(f'Average transcription word count: {avg_transcription_word_count}')\n",
    "print(f'Average transcription char count: {avg_transcription_char_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average tts_time column\n",
    "avg_tts_time = df_tts_es['tts_time'].mean()\n",
    "print(f'Average tts time: {avg_tts_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average tts_time column relative to the number of words\n",
    "df_tts_es['tts_time_per_word'] = df_tts_es['tts_time'] / df_tts_es['transcription_word_count']\n",
    "avg_tts_time_per_word = df_tts_es['tts_time_per_word'].mean()\n",
    "print(f'Average tts time per word: {avg_tts_time_per_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average tts_time column relative to the number of characters\n",
    "df_tts_es['tts_time_per_char'] = df_tts_es['tts_time'] / df_tts_es['transcription_char_count']\n",
    "avg_tts_time_per_char = df_tts_es['tts_time_per_char'].mean()\n",
    "print(f'Average tts time per char: {avg_tts_time_per_char}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the upper and lower bounds of the tts_time column\n",
    "upper_tts_time = df_tts_es['tts_time'].max()\n",
    "lower_tts_time = df_tts_es['tts_time'].min()\n",
    "print(f'Upper bound tts time: {upper_tts_time}')\n",
    "print(f'Lower bound tts time: {lower_tts_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_en = pd.read_csv(\"transcriptions_df_en.csv\")\n",
    "df_trans_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of words and characters in the \"Original Transcription\" and \"Predicted Transcription\" columns\n",
    "df_trans_en['original_transcription_word_count'] = df_trans_en['Original Transcription'].str.split().str.len()\n",
    "df_trans_en['predicted_transcription_word_count'] = df_trans_en['Predicted Transcription'].str.split().str.len()\n",
    "df_trans_en['original_transcription_char_count'] = df_trans_en['Original Transcription'].str.len()\n",
    "df_trans_en['predicted_transcription_char_count'] = df_trans_en['Predicted Transcription'].str.len()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average WER, CER, MER, RTF columns\n",
    "avg_wer = df_trans_en['WER'].mean()\n",
    "avg_cer = df_trans_en['CER'].mean()\n",
    "avg_mer = df_trans_en['MER'].mean()\n",
    "avg_rtf = df_trans_en['RTF'].mean()\n",
    "print(f'Average WER: {avg_wer}')\n",
    "print(f'Average CER: {avg_cer}')\n",
    "print(f'Average MER: {avg_mer}')\n",
    "print(f'Average RTF: {avg_rtf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average words and chars for original and predicted \n",
    "avg_original_transcription_word_count = df_trans_en['original_transcription_word_count'].mean()\n",
    "avg_predicted_transcription_word_count = df_trans_en['predicted_transcription_word_count'].mean()\n",
    "avg_original_transcription_char_count = df_trans_en['original_transcription_char_count'].mean()\n",
    "avg_predicted_transcription_char_count = df_trans_en['predicted_transcription_char_count'].mean()\n",
    "\n",
    "print(f'Average original transcription word count: {avg_original_transcription_word_count}')\n",
    "print(f'Average predicted transcription word count: {avg_predicted_transcription_word_count}')\n",
    "print(f'Average original transcription char count: {avg_original_transcription_char_count}')\n",
    "print(f'Average predicted transcription char count: {avg_predicted_transcription_char_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average percentage of overall predicted words relative to the actual words\n",
    "print(avg_original_transcription_word_count/avg_predicted_transcription_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average RTF \n",
    "avg_rtf = df_trans_en['RTF'].mean()\n",
    "print(f'Average RTF: {avg_rtf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average Prediction Time (s)\n",
    "avg_prediction=df_trans_en['Prediction Time (s)'].mean()\n",
    "print(f'Average Prediction Time (s): {avg_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the upper bound and the lower of the average time it takes to predict\n",
    "upper_prediction = df_trans_en['Prediction Time (s)'].max()\n",
    "lower_prediction = df_trans_en['Prediction Time (s)'].min()\n",
    "print(f'Upper bound time to predict: {upper_prediction}')\n",
    "print(f'Lower bound time to predict: {lower_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_es = pd.read_csv(\"transcriptions_df_es.csv\")\n",
    "df_trans_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of words and characters in the \"Original Transcription\" and \"Predicted Transcription\" columns\n",
    "df_trans_es['original_transcription_word_count'] = df_trans_es['Original Transcription'].str.split().str.len()\n",
    "df_trans_es['predicted_transcription_word_count'] = df_trans_es['Predicted Transcription'].str.split().str.len()\n",
    "df_trans_es['original_transcription_char_count'] = df_trans_es['Original Transcription'].str.len()\n",
    "df_trans_es['predicted_transcription_char_count'] = df_trans_es['Predicted Transcription'].str.len()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average WER, CER, MER, RTF columns\n",
    "avg_wer = df_trans_es['WER'].mean()\n",
    "avg_cer = df_trans_es['CER'].mean()\n",
    "avg_mer = df_trans_es['MER'].mean()\n",
    "avg_rtf = df_trans_es['RTF'].mean()\n",
    "print(f'Average WER: {avg_wer}')\n",
    "print(f'Average CER: {avg_cer}')\n",
    "print(f'Average MER: {avg_mer}')\n",
    "print(f'Average RTF: {avg_rtf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average words and chars for original and predicted \n",
    "avg_original_transcription_word_count = df_trans_es['original_transcription_word_count'].mean()\n",
    "avg_predicted_transcription_word_count = df_trans_es['predicted_transcription_word_count'].mean()\n",
    "avg_original_transcription_char_count = df_trans_es['original_transcription_char_count'].mean()\n",
    "avg_predicted_transcription_char_count = df_trans_es['predicted_transcription_char_count'].mean()\n",
    "\n",
    "print(f'Average original transcription word count: {avg_original_transcription_word_count}')\n",
    "print(f'Average predicted transcription word count: {avg_predicted_transcription_word_count}')\n",
    "print(f'Average original transcription char count: {avg_original_transcription_char_count}')\n",
    "print(f'Average predicted transcription char count: {avg_predicted_transcription_char_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_original_transcription_word_count/avg_predicted_transcription_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average RTF\n",
    "avg_rtf = df_trans_es['RTF'].mean()\n",
    "print(f'Average RTF: {avg_rtf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average Prediction Time (s)\n",
    "avg_prediction = df_trans_es['Prediction Time (s)'].mean()\n",
    "print(f'Average Prediction Time (s): {avg_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the upper bound and the lower of the average time it takes to predict\n",
    "upper_prediction = df_trans_es['Prediction Time (s)'].max()\n",
    "lower_prediction = df_trans_es['Prediction Time (s)'].min()\n",
    "print(f'Upper bound time to predict: {upper_prediction}')\n",
    "print(f'Lower bound time to predict: {lower_prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
