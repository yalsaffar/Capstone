{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "[2024-04-05 23:35:49,912] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-05 23:35:50,552] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-05 23:35:50,989] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0+ce78a63, git-hash=ce78a63, git-branch=master\n",
      "[2024-04-05 23:35:50,991] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-04-05 23:35:50,991] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-05 23:35:50,992] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-04-05 23:35:51,243] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spn\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from models.TTS_utils import load_manual_xtts_v2\n",
    "config_path = \"C:/tmp/xtts_ft/run/training/GPT_XTTS_FT-April-02-2024_05+08PM-0000000/config.json\"\n",
    "model_path = \"C:/tmp/xtts_ft/run/training/GPT_XTTS_FT-April-02-2024_05+08PM-0000000\"\n",
    "\n",
    "xtts_v2_model = load_manual_xtts_v2(config_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from models.TTS_utils import stream_prod\n",
    "stream_prod(xtts_v2_model, \"text.json\", \"audio_segments/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
