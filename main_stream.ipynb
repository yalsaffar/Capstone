{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spn\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\spn\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\spn\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "[2024-04-09 11:24:31,953] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-09 11:24:32,535] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-09 11:24:33,031] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0+ce78a63, git-hash=ce78a63, git-branch=master\n",
      "[2024-04-09 11:24:33,033] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-04-09 11:24:33,034] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-09 11:24:33,034] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-04-09 11:24:33,266] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spn\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from models.TTS_utils import load_manual_xtts_v2\n",
    "config_path = \"C:/tmp/xtts_ft/run/training/GPT_XTTS_FT-April-02-2024_05+08PM-0000000/config.json\"\n",
    "model_path = \"C:/tmp/xtts_ft/run/training/GPT_XTTS_FT-April-02-2024_05+08PM-0000000\"\n",
    "\n",
    "xtts_v2_model = load_manual_xtts_v2(config_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[(\"- I'm here to see you.\", 0, 'audio_segments\\\\segment_0.wav', 'results\\\\result_0.wav', 'en')]\n",
      "Processing text 0: - I'm here to see you.\n",
      "Processing text 0: en\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spn\\anaconda3\\envs\\capstone\\Lib\\site-packages\\TTS\\tts\\layers\\xtts\\stream_generator.py:138: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to first chunk: 1.2503578662872314\n",
      "Received chunk 0 of audio length 44544\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_0.wav\n",
      "Total audio length: 0\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('A more poseí', 3, 'audio_segments\\\\segment_3.wav', 'results\\\\result_3.wav', 'en')]\n",
      "Processing text 3: A more poseí\n",
      "Processing text 3: en\n",
      "3\n",
      "Time to first chunk: 0.6883454322814941\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_3.wav\n",
      "Received chunk 1 of audio length 5632\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_3.wav\n",
      "Total audio length: 5632\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('Socios speak English.', 7, 'audio_segments\\\\segment_7.wav', 'results\\\\result_7.wav', 'en')]\n",
      "Processing text 7: Socios speak English.\n",
      "Processing text 7: en\n",
      "7\n",
      "Time to first chunk: 0.6143834590911865\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_7.wav\n",
      "Received chunk 1 of audio length 12288\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_7.wav\n",
      "Total audio length: 12288\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[(\"I need to speak Spanish if you'll translate me to English.\", 9, 'audio_segments\\\\segment_9.wav', 'results\\\\result_9.wav', 'en')]\n",
      "Processing text 9: I need to speak Spanish if you'll translate me to English.\n",
      "Processing text 9: en\n",
      "9\n",
      "Time to first chunk: 0.6030657291412354\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_9.wav\n",
      "Received chunk 1 of audio length 39168\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_9.wav\n",
      "Total audio length: 39168\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "[(\"Homero needs to speak Spanish if you're going to translate me there.\", 10, 'audio_segments\\\\segment_10.wav', 'results\\\\result_10.wav', 'en')]\n",
      "Processing text 10: Homero needs to speak Spanish if you're going to translate me there.\n",
      "Processing text 10: en\n",
      "10\n",
      "Time to first chunk: 0.5695912837982178\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_10.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_10.wav\n",
      "Received chunk 2 of audio length 21248\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_10.wav\n",
      "Total audio length: 113664\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "[(\"It's very good, it's very good.\", 13, 'audio_segments\\\\segment_13.wav', 'results\\\\result_13.wav', 'en')]\n",
      "Processing text 13: It's very good, it's very good.\n",
      "Processing text 13: en\n",
      "13\n",
      "Time to first chunk: 0.5972654819488525\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_13.wav\n",
      "Received chunk 1 of audio length 25600\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_13.wav\n",
      "Total audio length: 71168\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('He speaks Spanish very well and translates it very easily, very honestly.', 16, 'audio_segments\\\\segment_16.wav', 'results\\\\result_16.wav', 'en')]\n",
      "Processing text 16: He speaks Spanish very well and translates it very easily, very honestly.\n",
      "Processing text 16: en\n",
      "16\n",
      "Time to first chunk: 0.6091609001159668\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_16.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_16.wav\n",
      "Received chunk 2 of audio length 7936\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_16.wav\n",
      "Total audio length: 54784\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "[(\"I'm going to go to the bathroom.\", 17, 'audio_segments\\\\segment_17.wav', 'results\\\\result_17.wav', 'en')]\n",
      "Processing text 17: I'm going to go to the bathroom.\n",
      "Processing text 17: en\n",
      "17\n",
      "Time to first chunk: 0.5772953033447266\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_17.wav\n",
      "Received chunk 1 of audio length 2304\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_17.wav\n",
      "Total audio length: 47872\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[(\"You're perfectly gorgeous.\", 20, 'audio_segments\\\\segment_20.wav', 'results\\\\result_20.wav', 'en')]\n",
      "Processing text 20: You're perfectly gorgeous.\n",
      "Processing text 20: en\n",
      "20\n",
      "Time to first chunk: 0.5528318881988525\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_20.wav\n",
      "Received chunk 1 of audio length 30208\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_20.wav\n",
      "Total audio length: 30208\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[(\"I want to see if you can do all the words in Spanish I can't speak very well, but I want to know if I\", 25, 'audio_segments\\\\segment_25.wav', 'results\\\\result_25.wav', 'en')]\n",
      "Processing text 25: I want to see if you can do all the words in Spanish I can't speak very well, but I want to know if I\n",
      "Processing text 25: en\n",
      "25\n",
      "Time to first chunk: 0.5598139762878418\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_25.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_25.wav\n",
      "Received chunk 2 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_25.wav\n",
      "Received chunk 3 of audio length 2304\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_3_25.wav\n",
      "Total audio length: 96000\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('Isley Hockey is a three-point shooter.', 28, 'audio_segments\\\\segment_28.wav', 'results\\\\result_28.wav', 'en')]\n",
      "Processing text 28: Isley Hockey is a three-point shooter.\n",
      "Processing text 28: en\n",
      "28\n",
      "Time to first chunk: 0.57059645652771\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_28.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_28.wav\n",
      "Received chunk 2 of audio length 14592\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_28.wav\n",
      "Total audio length: 61440\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('El barro se ve muy sexy hoy tengo que decir', 1, 'audio_segments\\\\segment_1.wav', 'results\\\\result_1.wav', 'es')]\n",
      "Processing text 1: El barro se ve muy sexy hoy tengo que decir\n",
      "Processing text 1: es\n",
      "1\n",
      "Time to first chunk: 0.6486434936523438\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_1.wav\n",
      "Received chunk 1 of audio length 26880\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_1.wav\n",
      "Total audio length: 26880\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('Espera, ¿qué dice uno?', 3, 'audio_segments\\\\segment_3.wav', 'results\\\\result_3.wav', 'es')]\n",
      "Processing text 3: Espera, ¿qué dice uno?\n",
      "Processing text 3: es\n",
      "3\n",
      "Time to first chunk: 0.5648462772369385\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_3.wav\n",
      "Received chunk 1 of audio length 2304\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_3.wav\n",
      "Total audio length: 2304\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('¿El modelo entiende como si yo dijera palabras muy complicadas como ubicua o alguna mierda así', 8, 'audio_segments\\\\segment_8.wav', 'results\\\\result_8.wav', 'es')]\n",
      "Processing text 8: ¿El modelo entiende como si yo dijera palabras muy complicadas como ubicua o alguna mierda así\n",
      "Processing text 8: es\n",
      "8\n",
      "Time to first chunk: 0.555600643157959\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_8.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_8.wav\n",
      "Received chunk 2 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_8.wav\n",
      "Received chunk 3 of audio length 42496\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_3_8.wav\n",
      "Total audio length: 136192\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('No puedo pensar en nada que decir ahora mismo pero el modelo es muy bueno el hecho de que lo haga tan rápido es bastante pking', 15, 'audio_segments\\\\segment_15.wav', 'results\\\\result_15.wav', 'es')]\n",
      "Processing text 15: No puedo pensar en nada que decir ahora mismo pero el modelo es muy bueno el hecho de que lo haga tan rápido es bastante pking\n",
      "Processing text 15: es\n",
      "15\n",
      "Time to first chunk: 0.5603511333465576\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_15.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_15.wav\n",
      "Received chunk 2 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_15.wav\n",
      "Received chunk 3 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_3_15.wav\n",
      "Received chunk 4 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_4_15.wav\n",
      "Received chunk 5 of audio length 18944\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_5_15.wav\n",
      "Total audio length: 206336\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('No te detecta a ti, sólo me detecta a mí. ¿Cómo funciona eso?', 27, 'audio_segments\\\\segment_27.wav', 'results\\\\result_27.wav', 'es')]\n",
      "Processing text 27: No te detecta a ti, sólo me detecta a mí. ¿Cómo funciona eso?\n",
      "Processing text 27: es\n",
      "27\n",
      "Time to first chunk: 0.5907108783721924\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_27.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_27.wav\n",
      "Received chunk 2 of audio length 34560\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_27.wav\n",
      "Total audio length: 81408\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('No es lo mismo así que sólo está bien así que es así que usted es que implementar que también personalizado lo que cómo funciona esto', 28, 'audio_segments\\\\segment_28.wav', 'results\\\\result_28.wav', 'es')]\n",
      "Processing text 28: No es lo mismo así que sólo está bien así que es así que usted es que implementar que también personalizado lo que cómo funciona esto\n",
      "Processing text 28: es\n",
      "28\n",
      "Time to first chunk: 0.5415678024291992\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_28.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_28.wav\n",
      "Received chunk 2 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_28.wav\n",
      "Received chunk 3 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_3_28.wav\n",
      "Received chunk 4 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_4_28.wav\n",
      "Received chunk 5 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_5_28.wav\n",
      "Received chunk 6 of audio length 26624\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_6_28.wav\n",
      "Total audio length: 260864\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('Como si fuéramos honestos no suena nada como lo que yo sonaría si hablara español', 32, 'audio_segments\\\\segment_32.wav', 'results\\\\result_32.wav', 'es')]\n",
      "Processing text 32: Como si fuéramos honestos no suena nada como lo que yo sonaría si hablara español\n",
      "Processing text 32: es\n",
      "32\n",
      "Time to first chunk: 0.5893125534057617\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_32.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_32.wav\n",
      "Received chunk 2 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_32.wav\n",
      "Received chunk 3 of audio length 2304\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_3_32.wav\n",
      "Total audio length: 96000\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('Puedo oírlo puedo oírlo puedo oírlo pero no es muy', 34, 'audio_segments\\\\segment_34.wav', 'results\\\\result_34.wav', 'es'), ('No suena muy natural.', 35, 'audio_segments\\\\segment_35.wav', 'results\\\\result_35.wav', 'es')]\n",
      "Processing text 34: Puedo oírlo puedo oírlo puedo oírlo pero no es muy\n",
      "Processing text 34: es\n",
      "Processing text 35: No suena muy natural.\n",
      "Processing text 35: es\n",
      "34\n",
      "Time to first chunk: 0.5943710803985596\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_34.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_34.wav\n",
      "Received chunk 2 of audio length 31232\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_34.wav\n",
      "Total audio length: 78080\n",
      "Audio playback finished.\n",
      "35\n",
      "Time to first chunk: 0.9536731243133545\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_35.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_35.wav\n",
      "Received chunk 2 of audio length 9984\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_35.wav\n",
      "Total audio length: 102400\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "[('Sueno muy español como yo. Sueno mal español.', 38, 'audio_segments\\\\segment_38.wav', 'results\\\\result_38.wav', 'es')]\n",
      "Processing text 38: Sueno muy español como yo. Sueno mal español.\n",
      "Processing text 38: es\n",
      "38\n",
      "Time to first chunk: 0.603161096572876\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_38.wav\n",
      "Received chunk 1 of audio length 46848\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_38.wav\n",
      "Received chunk 2 of audio length 24576\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_2_38.wav\n",
      "Total audio length: 116992\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "[('Así que si pudieras de alguna manera mi acento', 41, 'audio_segments\\\\segment_41.wav', 'results\\\\result_41.wav', 'es')]\n",
      "Processing text 41: Así que si pudieras de alguna manera mi acento\n",
      "Processing text 41: es\n",
      "41\n",
      "Time to first chunk: 0.55525803565979\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_41.wav\n",
      "Received chunk 1 of audio length 35840\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_41.wav\n",
      "Total audio length: 81408\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('- ¿ Qué?', 43, 'audio_segments\\\\segment_43.wav', 'results\\\\result_43.wav', 'es')]\n",
      "Processing text 43: - ¿ Qué?\n",
      "Processing text 43: es\n",
      "43\n",
      "Time to first chunk: 0.5691916942596436\n",
      "Received chunk 0 of audio length 29952\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_43.wav\n",
      "Total audio length: 0\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "[('Yo sé que piensas mi y mi opview', 44, 'audio_segments\\\\segment_44.wav', 'results\\\\result_44.wav', 'es')]\n",
      "Processing text 44: Yo sé que piensas mi y mi opview\n",
      "Processing text 44: es\n",
      "44\n",
      "Time to first chunk: 0.6520802974700928\n",
      "Received chunk 0 of audio length 45568\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_0_44.wav\n",
      "Received chunk 1 of audio length 22272\n",
      "24000\n",
      "Chunk saved as audio_segments/chunk_1_44.wav\n",
      "Total audio length: 22272\n",
      "Audio playback finished.\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n",
      "Inference...\n",
      "No more text to process\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTTS_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stream_prod\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstream_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtts_v2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecord_temp.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_segments/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spn\\Desktop\\IE_Last_year\\main_capstone\\models\\TTS_utils.py:186\u001b[0m, in \u001b[0;36mstream_prod\u001b[1;34m(model, json_path, directory_path)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m  \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mstream_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming finished\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\spn\\Desktop\\IE_Last_year\\main_capstone\\models\\TTS_utils.py:186\u001b[0m, in \u001b[0;36mstream_prod\u001b[1;34m(model, json_path, directory_path)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m  \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mstream_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming finished\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "    \u001b[1;31m[... skipping similar frames: stream_prod at line 186 (238 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\spn\\Desktop\\IE_Last_year\\main_capstone\\models\\TTS_utils.py:186\u001b[0m, in \u001b[0;36mstream_prod\u001b[1;34m(model, json_path, directory_path)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m  \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mstream_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming finished\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\spn\\Desktop\\IE_Last_year\\main_capstone\\models\\TTS_utils.py:185\u001b[0m, in \u001b[0;36mstream_prod\u001b[1;34m(model, json_path, directory_path)\u001b[0m\n\u001b[0;32m    183\u001b[0m results \u001b[38;5;241m=\u001b[39m streamer\u001b[38;5;241m.\u001b[39minference_and_play(json_path, directory_path)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m  \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    186\u001b[0m     stream_prod(model, json_path, directory_path)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming finished\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.TTS_utils import stream_prod\n",
    "stream_prod(xtts_v2_model, \"record_temp.json\", \"audio_segments/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
